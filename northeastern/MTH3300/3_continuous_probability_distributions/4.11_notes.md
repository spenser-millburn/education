# [4.11 - Central Limit Theorem] 

#### Central Limit Theorem (CLT)

- **Mean and Variance:** For any sample mean $X$, $\mu_X = \mu$ and $\sigma^2_X = \sigma^2/n$. The sum of the sample items, $S_n$, equates to the sample mean multiplied by the number of items $n$: $S_n = nX$. Therefore, $\mu_{S_n} = n\mu$ and $\sigma^2_{S_n} = n^2 \sigma^2/n = n\sigma^2$.

- **Approximation to Normal Distribution:** 
  - The CLT states that $X$ and $S_n$ are approximately normally distributed if the sample size, $n$, is sufficiently large.
  - "Large enough" sample size varies: symmetric distributions may need smaller $n$, while skewed distributions require larger $n$.
  - Empirically, a sample size of 30 or more is generally adequate for the normal approximation.
  
- **When Sample Size Exceeds 30:** 
  - The CLT approximation is considered effective for most population types when $n > 30$.

#### Commonly Used Distributions

The concept of distribution forms the base for understanding probability in statistics. Distributions can be of two types:
- **Probability Density Function (PDF)** for continuous random variables.
- **Probability Mass Function (PMF)** for discrete random variables.

#### The Central Limit Theorem (CLT)

The Central Limit Theorem is a fundamental theorem in statistics which states that the distribution of sample means approximates a normal distribution as the sample size becomes large, regardless of the shape of the population distribution. This theorem applies to both discrete and continuous distributions.

- **Sample Size Considerations**: 
  - For nearly symmetric distributions, the normal approximation is effective even with smaller sample sizes (e.g., 5).
  - For skewed distributions, larger sample sizes (e.g., 30) are typically  required to achieve a good approximation to the normal distribution.

#### Use of Mean and Variance

In many statistical applications, particularly those involving the Central Limit Theorem, knowledge of the mean and variance of the population is sufficient. The detailed probability distribution (PMF or PDF) is not required to utilize this theorem in practical applications such as hypothesis testing or constructing confidence intervals.

#### Central Limit Theorem (CLT)
The Central Limit Theorem states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size gets larger, regardless of the shape of the population distribution, given that the samples are independent and identically distributed (i.i.d.).

- **Standard parameters:** If the population has mean ($\mu$) and variance ($\sigma^2$), and if the sample size is $n$, then the sample mean $\bar{X}$ will approximately follow a normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$, i.e., $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$.

#### Application of CLT in Statistical Inference
CLT is widely used to find probabilities concerning sample means and sums in inferential statistics. For large samples, we can use the normal distribution model for these statistics even if the population from which the sample is drawn is not normally distributed.

#### Linear Combinations of Independent Normal Variables
Linear combinations of independent normal random variables result in a distribution that is also normally distributed. This principle can be extended through CLT to apply to:
- Sums of sample values
- Sample means

This is particularly useful in practical applications like determining the distribution of total times, sums, or averages in manufacturing processes, survey analysis, etc.

#### Normal Distribution of Sum of Independent Random Variables
- If $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$ are independent, then $Z = X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$.
- For the given case:
  - $SX \sim N(26, 0.65)$
  - $SY \sim N(29.25, 1.4625)$
  - Combined $T = SX + SY \sim N(55.25, 2.1125)$

#### Computing Probabilities Using Z-Scores
- To find $P(a < T < b)$, compute the z-scores:
  - $z = \frac{x - \mu}{\sigma}$
- For the given case:
  - $P(50 < T < 55) = 0.4323$ by using z-scores and looking up standard normal probabilities.

#### Normal Approximation to the Binomial Distribution
- If $X \sim \text{Bin}(n, p)$, then $X$ can be approximated by a normal distribution $N(np, np(1-p))$ if $np > 10$ and $n(1-p) > 10$.
- This approximation is reasonable when both $np$ and $n(1-p)$ are larger, providing a more accurate approximation with increasing $n$.

#### Normal Approximation to the Binomial Distribution
The binomial distribution with parameters $n$ and $p$ can be approximated by the normal distribution when $n$ is large enough. This approximation is generally represented as:
$$
\hat{p} \sim N\left(p, \frac{p(1-p)}{n}\right)
$$
where $\hat{p}$ is the sample proportion, and $N(\mu, \sigma^2)$ denotes the normal distribution with mean $\mu$ and variance $\sigma^2$. 

#### Continuity Correction
The continuity correction helps improve the accuracy when approximating a discrete distribution (like binomial) with a continuous one (like normal). It involves adjusting the range of values considered by 0.5 units on each side. For example, if you intend to find $P(a \leq X \leq b)$ for a binomial variable $X$, under normal approximation, you would calculate $P\left(a - 0.5 < Y < b + 0.5\right)$ where $Y$ is a normal random variable with the same mean and variance as $X$.

#### Continuity Correction in Normal Approximation of Binomial Distribution

To approximate the probability of a discrete random variable $X$ within a certain range using the normal distribution, consider whether endpoints should be included (inclusive) or excluded (exclusive).

- **Inclusive Range** $(a \leq X \leq b)$: Use the continuity correction by slightly expanding the range, translating to calculating the area under the normal curve from $a-0.5$ to $b+0.5$.
- **Exclusive Range** $(a < X < b)$: Use the continuity correction by slightly reducing the range, translating to calculating the area under the normal curve from $a+0.5$ to $b-0.5$.

This method ensures that the approximation using the normal curve accounts appropriately for the nature of the discrete probability histogram, including or excluding the appropriate endpoints.

#### Normal Approximation to Binomial Distribution

- **Normal Approximation**: Used to approximate the probabilities of a binomial distribution $B(n, p)$ especially when $n$ is large. 
- **Standard Normal Distribution**: Represents $Z$ where $Z = \frac{X - \mu}{\sigma}$, $X$ follows binomial distribution, $\mu = np$, and $\sigma = \sqrt{np(1-p)}$.
- **Continuity Correction**: Important for improving the accuracy of the normal approximation; add or subtract 0.5 to discrete $X$, e.g., if calculating $P(X > k)$, use $P(X > k + 0.5)$.

#### Accuracy of the Continuity Correction

- It generally enhances the accuracy of the normal approximation to the binomial distribution.
- It may decrease the precision in scenarios where $n$ is large and $p$ is small, specifically in the tails of the distribution, due to the normal approximation not fully accounting for asymmetry.

#### Normal Approximation to the Poisson Distribution

- Applicable when the Poisson parameter $\lambda$ is large (usually $\lambda > 10$).
- Under the approximation, the Poisson distribution can be modeled as a normal distribution with $\mu = \lambda$ and $\sigma^2 = \lambda$.

#### Central Limit Theorem for Poisson Distribution

If $X \sim \text{Poisson}(\lambda)$, where $\lambda > 10$, then $X$ is also approximately normal, $X \sim N(\lambda, \lambda)$.

#### Use of Normal Distribution to Approximate Poisson

- The normal distribution can be used to approximate the Poisson distribution when $\lambda > 10$.

#### Continuity Correction for the Poisson Distribution

- The continuity correction can improve the normal approximation for the central part of the curve if used but may worsen it for tails.
- It is generally not used for the Poisson distribution when approximating with the normal distribution.

#### Standard Normal Distribution and its Use in Statistics

1. **The Standard Normal Distribution**:
   - A normal distribution with a mean ($\mu$) of 0 and a standard deviation ($\sigma$) of 1.
   - Represented as $N(0,1)$.

2. **Converting to Standard Normal**:
   - Convert a normally distributed random variable $X$ with mean $\mu$ and standard deviation $\sigma$ using $Z = \frac{X - \mu}{\sigma}$.
   - This process is called standardization and the resulting $Z$ follows a standard normal distribution.

3. **Probabilities from Normal Distribution**:
   - Calculate probabilities for a normal distribution using its cumulative distribution function (CDF).
   - For a standard normal distribution, probabilities can be found using Z-tables or software functions like `norm.cdf` in Python.

4. **Applications in Hypothesis Testing**:
   - Normal distributions are used to model errors and sample statistics leading to various hypothesis testing approaches such as Z-tests.

#### Percentiles in Statistics

1. **Percentiles of a Distribution**:
   - A percentile indicates the value below which a given percentage of observations in a group falls.
   - For example, the 25th percentile is the value below which 25% of the observations may be found.

#### Central Limit Theorem (CLT)

1. **Central Limit Theorem**:
   - States that the distribution of sample means approximates a normal distribution as the sample size becomes large, regardless of the shape of the population distribution.
   - The mean of the sample means will be equal to the population mean ($\mu$), and the standard deviation of the sample means will be $\sigma / \sqrt{n}$ (standard error).

#### Sampling and Estimations

1. **Sample Mean Estimation**:
   - The sample mean $\overline{X}$ is used to estimate the population mean $\mu$.
   - The estimated standard error of $\overline{X}$ is $\sigma / \sqrt{n}$.

2. **Handling Proportions**:
   - For a population with a known proportion $p$, the standard error of the proportion in a sample of size $n$ is $\sqrt{\frac{p(1-p)}{n}}$.

#### Statistical Probability Calculations

1. **Probability of Exceedance**:
   - Calculated using the CDF of the pertinent distribution, whether normal or any other theoretical distribution.

2. **Calculations involving Standard Error**:
   - Often involve finding the probability that a statistic derived from a sample deviates from the population parameter by a certain amount.

#### Decision Making under Uncertainty

1. **Setting Confidence or Probability Thresholds**:
   - In practical applications such as quality control or risk assessment, decisions on sample size or thresholds are guided by acceptable risk levels (e.g., $\alpha = 0.05$ for 5% risk).

In this context, the discussion of specific probabilities and percentiles would relate directly back to these foundational concepts applied to various distributions, most commonly the normal distribution due to its central role in statistics.

#### Central Limit Theorem
The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. This is provided the observations are independent.

#### Sampling Distribution of the Sample Mean
For a random sample of size $n$ drawn from a population with mean $\mu$ and standard deviation $\sigma$, the mean of the sampling distribution of the sample mean is $\mu$, and its standard deviation (standard error) is $\frac{\sigma}{\sqrt{n}}$.

#### Probability Calculations using the Normal Distribution
To find the probability related to a sample mean $\bar{X}$:
- Standardize $\bar{X}$ to form a Z-score, $Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$.
- Use the Z-score to find probabilities using the standard normal distribution.

#### Percentile Calculation in Context of Normal Distribution
- To find the $k$th percentile of the total time taken (or of any other normally distributed variable), use the standard normal distribution to find the Z-score corresponding to $k$ and then transform this Z-score back to the original measurement scale.

#### Sample Size Calculation for Desired Precision
The sample size needed to achieve a certain margin of error $E$ at a given confidence level for estimating a mean can be approximated (assuming a normal distribution of the sample mean) as:
$$ n = \left(\frac{Z \cdot \sigma}{E}\right)^2 $$
where $Z$ is the Z-value from the standard normal distribution corresponding to the desired confidence level.

These are the key concepts required to solve problems related to the sampling distribution of the sample mean, including calculating probabilities, percentiles, and necessary sample sizes to achieve desired precision in estimation.


#### Central Limit Theorem (CLT)

- **Mean and Variance:** For any sample mean $X$, $\mu_X = \mu$ and $\sigma^2_X = \sigma^2/n$. The sum of the sample items, $S_n$, equates to the sample mean multiplied by the number of items $n$: $S_n = nX$. Therefore, $\mu_{S_n} = n\mu$ and $\sigma^2_{S_n} = n^2 \sigma^2/n = n\sigma^2$.

- **Approximation to Normal Distribution:** 
  - The CLT states that $X$ and $S_n$ are approximately normally distributed if the sample size, $n$, is sufficiently large.
  - "Large enough" sample size varies: symmetric distributions may need smaller $n$, while skewed distributions require larger $n$.
  - Empirically, a sample size of 30 or more is generally adequate for the normal approximation.
  
- **When Sample Size Exceeds 30:** 
  - The CLT approximation is considered effective for most population types when $n > 30$.

#### Commonly Used Distributions

The concept of distribution forms the base for understanding probability in statistics. Distributions can be of two types:
- **Probability Density Function (PDF)** for continuous random variables.
- **Probability Mass Function (PMF)** for discrete random variables.

#### The Central Limit Theorem (CLT)

The Central Limit Theorem is a fundamental theorem in statistics which states that the distribution of sample means approximates a normal distribution as the sample size becomes large, regardless of the shape of the population distribution. This theorem applies to both discrete and continuous distributions.

- **Sample Size Considerations**: 
  - For nearly symmetric distributions, the normal approximation is effective even with smaller sample sizes (e.g., 5).
  - For skewed distributions, larger sample sizes (e.g., 30) are typically required to achieve a good approximation to the normal distribution.

#### Use of Mean and Variance

In many statistical applications, particularly those involving the Central Limit Theorem, knowledge of the mean and variance of the population is sufficient. The detailed probability distribution (PMF or PDF) is not required to utilize this theorem in practical applications such as hypothesis testing or constructing confidence intervals.

#### Central Limit Theorem (CLT)
The Central Limit Theorem states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size gets larger, regardless of the shape of the population distribution, given that the samples are independent and identically distributed (i.i.d.).

- **Standard parameters:** If the population has mean ($\mu$) and variance ($\sigma^2$), and if the sample size is $n$, then the sample mean $\bar{X}$ will approximately follow a normal distribution with mean $\mu$ and variance $\frac{\sigma^2}{n}$, i.e., $\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$.

#### Application of CLT in Statistical Inference
CLT is widely used to find probabilities concerning sample means and sums in inferential statistics. For large samples, we can use the normal distribution model for these statistics even if the population from which the sample is drawn is not normally distributed.

#### Linear Combinations of Independent Normal Variables
Linear combinations of independent normal random variables result in a distribution that is also normally distributed. This principle can be extended through CLT to apply to:
- Sums of sample values
- Sample means

This is particularly useful in practical applications like determining the distribution of total times, sums, or averages in manufacturing processes, survey analysis, etc.

#### Normal Distribution of Sum of Independent Random Variables
- If $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$ are independent, then $Z = X + Y \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$.
- For the given case:
  - $SX \sim N(26, 0.65)$
  - $SY \sim N(29.25, 1.4625)$
  - Combined $T = SX + SY \sim N(55.25, 2.1125)$

#### Computing Probabilities Using Z-Scores
- To find $P(a < T < b)$, compute the z-scores:
  - $z = \frac{x - \mu}{\sigma}$
- For the given case:
  - $P(50 < T < 55) = 0.4323$ by using z-scores and looking up standard normal probabilities.

#### Normal Approximation to the Binomial Distribution
- If $X \sim \text{Bin}(n, p)$, then $X$ can be approximated by a normal distribution $N(np, np(1-p))$ if $np > 10$ and $n(1-p) > 10$.
- This approximation is reasonable when both $np$ and $n(1-p)$ are larger, providing a more accurate approximation with increasing $n$.

#### Normal Approximation to the Binomial Distribution
The binomial distribution with parameters $n$ and $p$ can be approximated by the normal distribution when $n$ is large enough. This approximation is generally represented as:
$$
\hat{p} \sim N\left(p, \frac{p(1-p)}{n}\right)
$$
where $\hat{p}$ is the sample proportion, and $N(\mu, \sigma^2)$ denotes the normal distribution with mean $\mu$ and variance $\sigma^2$. 

#### Continuity Correction
The continuity correction helps improve the accuracy when approximating a discrete distribution (like binomial) with a continuous one (like normal). It involves adjusting the range of values considered by 0.5 units on each side. For example, if you intend to find $P(a \leq X \leq b)$ for a binomial variable $X$, under normal approximation, you would calculate $P\left(a - 0.5 < Y < b + 0.5\right)$ where $Y$ is a normal random variable with the same mean and variance as $X$.

#### Continuity Correction in Normal Approximation of Binomial Distribution

To approximate the probability of a discrete random variable $X$ within a certain range using the normal distribution, consider whether endpoints should be included (inclusive) or excluded (exclusive).

- **Inclusive Range** $(a \leq X \leq b)$: Use the continuity correction by slightly expanding the range, translating to calculating the area under the normal curve from $a-0.5$ to $b+0.5$.
- **Exclusive Range** $(a < X < b)$: Use the continuity correction by slightly reducing the range, translating to calculating the area under the normal curve from $a+0.5$ to $b-0.5$.

This method ensures that the approximation using the normal curve accounts appropriately for the nature of the discrete probability histogram, including or excluding the appropriate endpoints.

#### Normal Approximation to Binomial Distribution

- **Normal Approximation**: Used to approximate the probabilities of a binomial distribution $B(n, p)$ especially when $n$ is large. 
- **Standard Normal Distribution**: Represents $Z$ where $Z = \frac{X - \mu}{\sigma}$, $X$ follows binomial distribution, $\mu = np$, and $\sigma = \sqrt{np(1-p)}$.
- **Continuity Correction**: Important for improving the accuracy of the normal approximation; add or subtract 0.5 to discrete $X$, e.g., if calculating $P(X > k)$, use $P(X > k + 0.5)$.

#### Accuracy of the Continuity Correction

- It generally enhances the accuracy of the normal approximation to the binomial distribution.
- It may decrease the precision in scenarios where $n$ is large and $p$ is small, specifically in the tails of the distribution, due to the normal approximation not fully accounting for asymmetry.

#### Normal Approximation to the Poisson Distribution

- Applicable when the Poisson parameter $\lambda$ is large (usually $\lambda > 10$).
- Under the approximation, the Poisson distribution can be modeled as a normal distribution with $\mu = \lambda$ and $\sigma^2 = \lambda$.

#### Central Limit Theorem for Poisson Distribution

If $X \sim \text{Poisson}(\lambda)$, where $\lambda > 10$, then $X$ is also approximately normal, $X \sim N(\lambda, \lambda)$.

#### Use of Normal Distribution to Approximate Poisson

- The normal distribution can be used to approximate the Poisson distribution when $\lambda > 10$.

#### Continuity Correction for the Poisson Distribution

- The continuity correction can improve the normal approximation for the central part of the curve if used but may worsen it for tails.
- It is generally not used for the Poisson distribution when approximating with the normal distribution.

#### Standard Normal Distribution and its Use in Statistics

1. **The Standard Normal Distribution**:
   - A normal distribution with a mean ($\mu$) of 0 and a standard deviation ($\sigma$) of 1.
   - Represented as $N(0,1)$.

2. **Converting to Standard Normal**:
   - Convert a normally distributed random variable $X$ with mean $\mu$ and standard deviation $\sigma$ using $Z = \frac{X - \mu}{\sigma}$.
   - This process is called standardization and the resulting $Z$ follows a standard normal distribution.

3. **Probabilities from Normal Distribution**:
   - Calculate probabilities for a normal distribution using its cumulative distribution function (CDF).
   - For a standard normal distribution, probabilities can be found using Z-tables or software functions like `norm.cdf` in Python.

4. **Applications in Hypothesis Testing**:
   - Normal distributions are used to model errors and sample statistics leading to various hypothesis testing approaches such as Z-tests.

#### Percentiles in Statistics

1. **Percentiles of a Distribution**:
   - A percentile indicates the value below which a given percentage of observations in a group falls.
   - For example, the 25th percentile is the value below which 25% of the observations may be found.

#### Central Limit Theorem (CLT)

1. **Central Limit Theorem**:
   - States that the distribution of sample means approximates a normal distribution as the sample size becomes large, regardless of the shape of the population distribution.
   - The mean of the sample means will be equal to the population mean ($\mu$), and the standard deviation of the sample means will be $\sigma / \sqrt{n}$ (standard error).

#### Sampling and Estimations

1. **Sample Mean Estimation**:
   - The sample mean $\overline{X}$ is used to estimate the population mean $\mu$.
   - The estimated standard error of $\overline{X}$ is $\sigma / \sqrt{n}$.

2. **Handling Proportions**:
   - For a population with a known proportion $p$, the standard error of the proportion in a sample of size $n$ is $\sqrt{\frac{p(1-p)}{n}}$.

#### Statistical Probability Calculations

1. **Probability of Exceedance**:
   - Calculated using the CDF of the pertinent distribution, whether normal or any other theoretical distribution.

2. **Calculations involving Standard Error**:
   - Often involve finding the probability that a statistic derived from a sample deviates from the population parameter by a certain amount.

#### Decision Making under Uncertainty

1. **Setting Confidence or Probability Thresholds**:
   - In practical applications such as quality control or risk assessment, decisions on sample size or thresholds are guided by acceptable risk levels (e.g., $\alpha = 0.05$ for 5% risk).

In this context, the discussion of specific probabilities and percentiles would relate directly back to these foundational concepts applied to various distributions, most commonly the normal distribution due to its central role in statistics.

#### Central Limit Theorem
The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. This is provided the observations are independent.

#### Sampling Distribution of the Sample Mean
For a random sample of size $n$ drawn from a population with mean $\mu$ and standard deviation $\sigma$, the mean of the sampling distribution of the sample mean is $\mu$, and its standard deviation (standard error) is $\frac{\sigma}{\sqrt{n}}$.

#### Probability Calculations using the Normal Distribution
To find the probability related to a sample mean $\bar{X}$:
- Standardize $\bar{X}$ to form a Z-score, $Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}$.
- Use the Z-score to find probabilities using the standard normal distribution.

#### Percentile Calculation in Context of Normal Distribution
- To find the $k$th percentile of the total time taken (or of any other normally distributed variable), use the standard normal distribution to find the Z-score corresponding to $k$ and then transform this Z-score back to the original measurement scale.

#### Sample Size Calculation for Desired Precision
The sample size needed to achieve a certain margin of error $E$ at a given confidence level for estimating a mean can be approximated (assuming a normal distribution of the sample mean) as:
$$ n = \left(\frac{Z \cdot \sigma}{E}\right)^2 $$
where $Z$ is the Z-value from the standard normal distribution corresponding to the desired confidence level.

These are the key concepts required to solve problems related to the sampling distribution of the sample mean, including calculating probabilities, percentiles, and necessary sample sizes to achieve desired precision in estimation.



