#### Commonly Used Distributions
The textbook chapter discusses several important statistical distributions. Here, we focus on the Weibull distribution and the normal distribution used in examples within this excerpt.

#### Weibull Distribution
The Weibull distribution is a continuous probability distribution used extensively in reliability engineering and failure analysis. It is defined by two parameters:
- $\alpha$ (shape parameter)
- $\beta$ (scale parameter)

The probability density function (pdf) of the Weibull distribution is given by:
$$
f(x;\alpha,\beta) = \frac{\alpha}{\beta} \left(\frac{x}{\beta}\right)^{\alpha - 1} e^{-\left(\frac{x}{\beta}\right)^\alpha} \quad \text{for } x \geq 0
$$

#### Normal Distribution
The normal distribution, also known as the Gaussian distribution, is another continuous probability distribution that is symmetric around its mean, showing that data near the mean are more frequent in occurrence than data far from the mean.

The normal distribution is defined by two parameters:
- $\mu$ (mean of the distribution)
- $\sigma$ (standard deviation of the distribution)

The probability density function (pdf) of the normal distribution is:
$$
f(x; \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

These distributions are used to model various real-world phenomena and are pivotal in defining the probabilities of different outcomes occurring under specified conditions.

#### Poisson Distribution

The Poisson distribution is often used to model the number of times an event happens in a fixed interval of time or space. The parameter $\lambda$ represents the average number of events in that interval.

- **Probability Mass Function (PMF)**: 
  $$
  P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}
  $$ 
  where $X$ is the number of events, $k$ is a nonnegative integer, and $\lambda$ is the average rate of events.

#### Poisson Distribution Application to Website Hits

For a website with hits following a Poisson distribution with a mean rate of $\lambda = 20$ hits per hour:
  
- **Modeling Hits Over Extended Time**:
  For $5$ hours, the effective $\lambda$ becomes $100$ (since $20 \times 5 = 100$).

- **Calculation of Probabilities**:
  - $P(X \leq k)$ can be calculated using the cumulative distribution function (CDF) of the Poisson distribution.

#### Important Concepts of Statistical Significance in Observations

When observing values like total weight or hit counts, determining the typicality and extremeness involves checking:

- The probability of observing a value as extreme or more extreme than the observed.
- If this probability is exceptionally low (commonly below a threshold such as 0.05), it might be statistically significant evidence against the presumed conditions (e.g., the assumed mean or distribution). 

These concepts are applied to assess whether observed data significantly deviates from what is expected under a claimed hypothesis or model, helping to validate or refute the claims.

#### Extreme Value Distribution
- **Cumulative Distribution Function (CDF):** 
  $$ F(x) = e^{-e^{-x}} $$

#### Pareto Distribution
- **Probability Density Function (PDF):**
  $$ f(x) = \begin{cases} 
  \frac{\theta r}{x^{r+1}} & x \geq \theta \\
  0 & x < \theta 
  \end{cases} $$
- **Parameters:**
  - $\theta > 0$
  - $r > 0$
- **Cumulative Distribution Function (CDF):**
  $$ F(x) = 1 - \left(\frac{\theta}{x}\right)^r \quad \text{for } x \geq \theta $$
- **Expected Value ($\mu_X$):**
  - For $r > 1$: 
    $$ \mu_X = \frac{\theta r}{r-1} $$
  - For $r \leq 1$, $\mu_X$ does not exist.
- **Variance ($\sigma^2_X$):**
  - For $r > 2$: 
    $$ \sigma^2_X = \frac{\theta^2 r}{(r-1)^2(r-2)} $$
  - For $r \leq 2$, $\sigma^2_X$ does not exist.

#### Logistic Distribution
- **Cumulative Distribution Function (CDF):**
  $$ F(x) = \frac{1}{1 + e^{-\frac{x-\alpha}{\beta}}} $$
- **Parameters:**
  - $\alpha \in \mathbb{R}$ (location parameter)
  - $\beta > 0$ (scale parameter)
- **Probability Density Function (PDF):**
  $$ f_X(x) = \frac{e^{-\frac{x-\alpha}{\beta}}}{\beta \left(1 + e^{-\frac{x-\alpha}{\beta}}\right)^2} $$
- Due to the properties of the logistic function, $f_X(x)$ is symmetric around $x = \alpha$.

#### Symmetry and Mean of a Random Variable

When a probability density function $f_X(x)$ of a random variable $X$ is symmetric around a point $\alpha$, meaning $f_X(\alpha - x) = f_X(\alpha + x)$ for all $x$, it indicates that $\alpha$ is the mean ($\mu_X$) of the distribution.
- This symmetry suggests that the distribution has equal weights on either side of $\alpha$, leading to $\alpha$ being the center of mass or the mean of the distribution: $$ \mu_X = \alpha $$

#### Radioactive Particle Emission and Mixed Exponential Distribution

For a scenario involving two radioactive masses with different emission rates and selection probabilities, if the time $X$ until the first particle is emitted follows a mixed exponential distribution, the probability density function (pdf) is given by:
$$ f(x) = \begin{cases} 
p \lambda_1 e^{-\lambda_1 x} + (1-p) \lambda_2 e^{-\lambda_2 x} & \text{for } x \geq 0 \\
0 & \text{for } x < 0 
\end{cases} $$

* **Expectation ($\mu_X$):** 
  - The expected value or mean $\mu_X$ of $X$ can be calculated using:
    $$ \mu_X = \int_0^\infty x[p \lambda_1 e^{-\lambda_1 x} + (1-p) \lambda_2 e^{-\lambda_2 x}] \, dx $$
  - Each term in the sum can be integrated separately using the known result for the expected value of exponential distribution:
    $$ E(X) = \frac{p}{\lambda_1} + \frac{1-p}{\lambda_2} $$

* **Cumulative Distribution Function (CDF):**
  - The CDF, $F_X(x)$, of $X$ can be derived by integrating the pdf:
    $$ F_x(x) = \int_0^x [p \lambda_1 e^{-\lambda_1 t} + (1-p) \lambda_2 e^{-\lambda_2 t}] \, dt $$
  - This results in:
    $$ F_X(x) = 1 - p e^{-\lambda_1 x} - (1-p) e^{-\lambda_2 x} $$

* **Calculation of Specific Probabilities:**
  - For given parameters and $x = 2$, $P(X \leq 2)$ can be computed directly using the CDF.

#### Geometric Distribution and Lack of Memory Property

* **Probability Calculation after $s$ Failures:**
  - For a geometrically distributed random variable $X \sim \text{Geom}(p)$:
    $$ P(X > s) = (1-p)^s $$
  - This is derived from the definition of the geometric distribution, where the probability of success (failure) on each trial is independent.

* **Lack of Memory Property:**
  - The geometric distribution exhibits the lack of memory property, which states:
    $$ P(X > s + t \,|\, X > s) = P(X > t) $$
  - This property can be shown by manipulating the conditional probability:
    $$ P(X > s + t \,|\, X >  s) = \frac{P(X > s+t)}{P(X > s)} = \frac{(1-p)^{s+t}}{(1-p)^s} = (1-p)^t $$

#### Lack of Memory Property in Probability
The lack of memory property is crucial in probability theory, particularly for exponential and geometric distributions. It guarantees that the probability distribution of the remaining lifespan or trials does not depend on any previously accumulated time or number of trials. This property is defined mathematically for an exponentially distributed variable $X$, such that for any $s, t \geq 0$:

$$
P(X > s + t \mid X > s) = P(X > t)
$$

#### Transforming Random Variables
When transforming random variables, it is important to understand how the distribution is affected by the transformation.

- **Linear Transformations:** If you have a random variable $X$ with a cumulative distribution function $F_X(x)$ and you define a new random variable $Y = aX + b$, the new cumulative distribution function $F_Y(y)$ can be resolved by substituting $(y-b)/a$ for $x$ in $F_X(x)$:
  $$
  F_Y(y) = F_X\left(\frac{y - b}{a}\right)
  $$

- **Exponential Distribution Transformation Example:**
  If $X \sim \text{Exp}(\lambda)$, then $Y = 7X$ for $Y$. The transformation affects the distribution parameters, converting it to $Y \sim \text{Exp}(\lambda / 7)$.

#### Binomial Distribution and Conditional Probability
For a binomial random variable $X \sim \text{Bin}(n, p)$, the probability of observing exactly $x$ successes in $n$ trials is given by:
$$
P(X = x) = \binom{n}{x} p^x (1-p)^{n-x}
$$
This can provide insights into conditional relationships within distribution, such as the probability ratio of successive events:
$$
\frac{P(X=x)}{P(X=x-1)} = \frac{n-x+1}{x} \cdot \frac{p}{1-p}
$$

#### Mean Area of a Randomly Broken Stick
When a stick of unit length is broken randomly, forming a rectangle, the mean area becomes an integral calculation over uniform probability. If the break point $X$ is uniformly distributed over $[0, 1]$, and $Y = 1 - X$, the area $A$ of the rectangle formed is $X \cdot Y = X \cdot (1-X)$. The expectation of $A$ is:
$$
E(A) = E(X \cdot (1 - X)) = \int_0^1 x(1 - x) \, dx
$$

These outlined concepts provide a sturdy foundation for understanding specific random processes, distributions transformations, and their implications in probabilistic modeling.

#### Binomial Distribution
- For a random variable $X \sim \text{Bin}(n,p)$, the most likely value of $X$ is the largest integer less than or equal to $np + p$.

#### Poisson Distribution
- For a random variable $X \sim \text{Poisson}(\lambda)$, the ratio $$\frac{P(X=x)}{P(X=x-1)} = \frac{\lambda}{x}$$ if $x$ is a positive integer.
- The most probable value for $X$ is the greatest integer less than or equal to $\lambda$.

#### Normal Distribution Transformations
- For $Z \sim N(0,1)$ and transformed variable $X = \sigma Z + \mu$, where $\mu$ is a constant and $\sigma > 0$:
  - The cumulative distribution function (CDF) of $X$ is given by:
    $$
    F_X(x) = \Phi \left(\frac{x-\mu}{\sigma}\right)
    $$
    where $\Phi$ represents the CDF of $Z$.
  - By differentiation, $X$ follows a normal distribution $N(\mu, \sigma^2)$.
- For another transformation where $X = -\sigma Z + \mu$, the computed CDF similarly verifies that $X \sim N(\mu, \sigma^2)$.

#### Point Estimates and Their Precision

A **point estimate** (e.g., the sample proportion $\hat{p}$) is used as an estimate of a parameter (e.g., a success probability $p$). It is important to note that:
- Point estimates almost never exactly equal the true values they are aiming to estimate.
- They can differ from the true value either by a little or by a lot depending on various factors.

#### Uncertainty in Point Estimates

The uncertainty or standard deviation of a point place gives an idea of how far off this estimate could likely be from the actual value. Understanding and quantifying this uncertainty is crucial for the usefulness of the point estimate.

#### Confidence Intervals

A **confidence interval** provides a range of values, derived from the point estimate, that is likely to contain the true parameter value. When the estimate comes from a normal distribution:
- A confidence interval can be calculated to provide more precise information about the extent to a point estimate approximates the parameter.
- This interval helps in understanding the reliability and precision of the estimate.

#### Confidence Intervals

Confidence intervals provide a range of values, estimated from data, that is likely to contain the true value of an unknown parameter. The interval has an associated confidence level that quantifies the level of confidence that the parameter lies within the interval. 

- **Definition**: A confidence interval for a parameter is an interval constructed from sample data, most commonly expressed as point estimate ± margin of error, where the margin of, error is based on the standard deviation of the estimate and the level of confidence desired.

- **Interpretation**: If an interval estimate for a parameter with a confidence level of 95% is (13.9, 14.1), it means that if we were to take many samples and construct the confidence interval in the same way from each sample, approximately 95% of these intervals will contain the true parameter value.

- **Significance of Standard Deviation in Confidence Intervals**: Standard deviation plays a crucial role in establishing the margin of error. The larger the standard deviation, the wider the confidence intervals will be, reflecting less precision in point estimates.

- **Levels of Confidence**: Common levels of confidence are 90%, 95%, and 99%. A 99.7% confidence level corresponds to approximately three standard deviations from the mean in a normal distribution, suggesting very high confidence in the resulting interval.

The chapter refers to methods for calculating confidence intervals for large samples, using the normal distribution assumptions, particularly when sample sizes are sufficient to invoke the central limit theorem. Such methods help quantify the uncertainty in estimates obtained from data.

#### Large-Sample Confidence Intervals for a Population Mean

When estimating a population mean $$ \mu $$ from a large sample, confidence intervals are used to indicate the range within which the true population mean is believed to fall with a certain level of confidence.

- **Definition of Confidence Interval:**
   The confidence interval for a population mean provides an estimated range of values which is likely to include the population mean. This interval is calculated based on the sample mean and the sample standard deviation.

- **Calculating the Confidence Interval:**
   Given the sample mean $$ \overline{X} $$, sample size $$ n $$, and sample standard deviation $$ s $$, the confidence interval for the population mean when the sample size is large (typically $$ n \geq 30 $$) is given by:
   $$
   \left(\overline{X} - z \frac{s}{\sqrt{n}}, \overline{X} + z \frac{s}{\sqrt{n}}\right)
   $$
   where $$ z $$ is the z-score corresponding to the desired confidence level (e.g., 1.96 for 95% confidence).

- **Interpretation:**
   A 95% confidence interval, for example, means that if the sampling procedure were repeated numerous times, approximately 95% of the confidence intervals generated would contain the true population mean.

- **Assumptions and Conditions:**
   - The sample should be a simple random sample from the population.
   - The sample size should be sufficiently large to rely on the Central Limit Theorem for the distribution of sample mean to be approximately normal.

#### Central Limit Theorem (CLT)
The Central Limit Theorem states that the distribution of the sample mean \( X \) from a large sample is approximately normal, regardless of the distribution of the original dataset. This normal distribution has:

- Mean \( \mu \)
- Standard deviation \( \sigma_X = \frac{\sigma}{\sqrt{n}} \)

where \( \sigma \) is the standard deviation of the original dataset and \( n \) is the sample size.

#### Normal Distribution and Confidence Intervals
For a normally distributed variable:

- **Mean:** \( \mu \)
- **Standard Deviation:** \( \sigma_X \)

The middle 95% of this distribution extends \( 1.96 \sigma_X \) on either side of the mean \( \mu \). Thus, a 95% confidence interval for the mean can be written as:

$$
\mu \pm 1.96 \sigma_X
$$

This interval captures the population mean \( \mu \) with a 95% probability, given the sample mean \( X \).


