### [ 4.9 ] - Notes

#### Principles of Point Estimation

- Mean Squared Error (MSE) is the sum of the squared bias and variance: $MSE = Bias^2 + Variance$

- The difference between the estimated value and the true value is called the error, denoted as $\hat{\theta} - \theta$

- MSE is the mean of the squared error, giving it its name

- Equations (4.53) and (4.54) yield identical results for MSE calculation; Equation (4.53) is often easier to use in practice

#### Example 4.67

Given $X \sim Bin(n,p)$ where $p$ is unknown, the MSE of $\hat{p} = X/n$ is computed by finding the bias and variance of $\hat{p}$ and using Equation (4.53).

- The bias of $\hat{p}$ is 0 and the variance is $p(1-p)/n$

- Therefore, the MSE is $0 + p(1-p)/n$, or $p(1-p)/n$

- In this example, since the estimator was unbiased, the MSE was equal to the variance of the estimator

#### Unbiased Estimator and Bias-Variance Trade-off

In statistics, an unbiased estimator can sometimes be adjusted to introduce a small bias but significantly reduce the variance, improving overall performance.

**Example**

Given a random sample $X_1, ..., X_n$ from a population with $N(\mu, \sigma^2)$ distribution, the sample variance is calculated as:

$s^2 = \frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n-1}$

Consider an alternative estimator:

$$
\hat{\sigma}^2 = \frac{\sum_{i=1}^{n}(X_i - \bar{X})^2}{n}
$$

Computing the bias, variance, and mean squared error of both estimators reveals that $\hat{\sigma}^2$ can have a smaller mean squared error than $s^2$.

**Solution**

- The bias of $\hat{\sigma}^2$ is $-\frac{\sigma^2}{n}$.
- The variance of $\hat{\sigma}^2$ is $\frac{2\sigma^4}{n-1}$.

#### Commonly Used Distributions

The variance is given by:
$ \hat{\sigma^2} = (\frac{n-1}{n})^2\sigma^2 $

The mean squared error of $\hat{\sigma^2}$ is:
$ MSE \hat{\sigma^2} = \left(\frac{\sigma^2}{n}\right)^2 + \frac{2(n-1)}{n^2}\sigma^4 \\
= \frac{2n-1}{n^2}\sigma^4 $

To show that $\hat{\sigma^2}$ has smaller mean squared error than $s^2$, we subtract:

$ MSE s^2 - MSE \hat{\sigma^2} = \frac{2\sigma^4}{n-1} - \frac{(2n-1)\sigma^4}{n^2} = \frac{3n-1}{n^2(n-1)} > 0 \quad \text{(since } n > 1 \text{)} $

#### The Method of Maximum Likelihood

The method of maximum likelihood aims to estimate a parameter by choosing the value that maximizes the likelihood of the observed data.-

For example, for $X \sim Bin(20, p)$ where $p$ is unknown and we observe $X=7$:
$ f(7;p) = \frac{20!}{7!13!}p^7(1-p)^{13} $

#### Likelihood Function and MLE

The likelihood function is denoted as $f(7;p)$ and represents the probability mass function as a function of the parameter $p$, with the data value 7 held constant. The maximum likelihood estimate ($\hat{p}$) is the value that maximizes the likelihood function.

#### Maximizing Likelihood Function

To find the maximum likelihood estimate, we maximize the natural logarithm of the likelihood function, which is denoted as $ln f(7;p)$. By setting the derivative of $ln f(7;p)$ with respect to $p$ equal to 0, we can find the maximum likelihood estimate.

#### Derivative and Setting Equal to 0

Taking the derivative with respect to $p$ and setting it equal to 0 gives us the expression: $\frac{7}{p} - \frac{13}{1-p} = 0$. Solving this equation helps us find the MLE for the parameter $p$.

#### Maximum Likelihood Estimation

The maximum likelihood estimate is given by:
$$ \hat{\theta}_{\text{MLE}} = \frac{7}{20} $$

For any observed value \( X \), the maximum likelihood estimator is :
$$ \hat{\theta}_{\text{MLE}} = \frac{X}{20} $$

#### Likelihood Function

The likelihood function can be a probability density function, probability mass function, or a joint probability density or mass function. It may involve multiple parameters and can be the joint density or mass function of independent random variables.

#### Definition of Maximum Likelihood Estimator

Let \( X_1, ..., X_n \) have joint probability density or probability mass function \( f(x_1, ..., x_n; \theta \)

#### Maximum Likelihood Estimation (MLE)

The maximum likelihood estimate ($\hat{\theta}_1, ..., \hat{\theta}_k$) for parameters ($\theta_1, ..., \theta_k$) is found by maximizing the likelihood function.

#### Maximum Likelihood Estimators

If the observed values $x_1, ..., x_n$ are substituted for the random variables $X_1, ..., X_n$, the resulting estimates ($\hat{\theta}_1, ..., \hat{\theta}_k$) are known as maximum likelihood estimators.

#### Notation

The abbreviation MLE is commonly used to refer to both the maximum likelihood estimate and the maximum likelihood estimator.

#### Example: MLE of $\lambda$ for Exp($\lambda$) Population

Given a random sample $X_1, ..., X_n$ from an exponential distribution Exp($\lambda$), the likelihood function for finding MLE of $\lambda$ is:

$f(x_1, ..., x_n; \lambda) = \lambda^n e^{-\lambda \sum x_i}$

The MLE $\hat{\lambda}$ maximizes this likelihood function by finding the value of $\lambda$ that maximizes the joint probability density function.

#### Maximum Likelihood Estimation (MLE)

$ f(x_1,..., x_n;\lambda) = \lambda^n e^{-\lambda}/\sum_{i=1}^{n}x_i $

- To maximize likelihood, it's easier to work with the logarithm:
$ \ln f(x_1,..., x_n;\lambda) = n\ln\lambda - \lambda n/\sum_{i=1}^{n}x_i $

- Derivative with respect to λ, set to 0 gives:
$ \frac{d}{d\lambda}\ln f = n/\lambda - n/\sum_{i=1}^{n}x_i = 0 $

- Solving for λ, we get the MLE:
$ \hat{\lambda} = n/\sum^n_{i=1} x_i = 1/x $

#### Properties of Maximum Likelihood Estimators

- MLEs have two desirable properties:
1. As sample size increases, bias converges to 0.
2. As sample size increases, variance converges to a minimum.

- Implies that with large sample sizes, bias of the MLE is negligible, and the variance is minimized.

#### Unbiased Estimator
- If an estimator is unbiased, then the mean of the estimator is equal to the true value.

#### Variance of an Estimator
- The variance of an estimator measures how close repeated values of the estimator are to each other.

#### Independent Variables Estimation
- For independent variables X1 and X2 with unknown mean μ and known variance σ^2=1:
  a. Let $\hat{\mu}_1 = \frac{X_1+X_2}{2}$
     - Find the bias, variance, and mean squared error of $\hat{\mu}_1$
  b. Let $\hat{\mu}_2 = \frac{X_1 + 2X_2}{3}$
     - Find the bias, variance, and mean squared error of $\hat{\mu}_2$

#### 1. Estimate Bias, Variance, and Mean Squared Error of $\hat{\mu}_3$

- **Bias**: Bias is the difference between the expected value of the estimator and the true parameter value.
- **Variance**: Variance measures how much the estimates for a given parameter vary as different samples are taken.
- **Mean Squared Error**: It is the average squared difference between the estimator and the true parameter value.

#### 2. Compare Mean Squared Error of $\hat{\mu}_3$ with $\hat{\mu}_1$

- Determine the values of $\mu$ for which the mean squared error of $\hat{\mu}_3$ is smaller than that of $\hat{\mu}_1$.

#### 3. Compare Mean Squared Error of $\hat{\mu}_3$ with $\hat{\mu}_2$

- Identify the values of $\mu$ where the mean squared error of $\hat{\mu}_3$ is less than the mean squared error of $\hat{\mu}_2$.

#### 4. Properties of $\hat{\sigma}^2_k$

- **Bias**: Compute the bias of $\hat{\sigma}^2_k$ in terms of the constant $k$.
- **Variance**: Calculate the variance of $\hat{\sigma}^2_k$ in terms of $k$.
- **Mean Squared Error**: Determine the mean squared error of $\hat{\sigma}^2_k$ in terms of $k$.
- **Minimization**: Find the value of $k$ that minimizes the mean squared error of $\hat{\sigma}^2_k$.

#### 5. Maximum Likelihood Estimation for $p$ in Geometric Distribution

- Find the Maximum Likelihood Estimator (MLE) of $p$ in a Geometric distribution denoted by $X \sim Geom(p)$.

#### 6. Estimation in Poisson Distribution

- Given a random sample $X_1, ..., X_n$ from a population with a Poisson distribution with parameter $\lambda$, determine the estimation.


#### MLE of λ

The maximum likelihood estimate (MLE) of $\lambda$.

#### Functional Invariance Property

Maximum likelihood estimates possess functional invariance: if $\hat{\theta}$ is the MLE of $\theta$, and $h(\theta)$ is any function of $\theta$, then $h(\hat{\theta})$ is the MLE of $h(\theta)$.

#### a) Binomial Distribution

Given $X \sim \text{Bin}(n, p)$ with $n$ known and $p$ unknown, find the MLE of the odds ratio $p/(1-p)$.

#### b) Geometric Distribution

Using the result from Exercise 5, find the MLE of the odds ratio $p/(1-p)$ if $X \sim \text{Geom}(p)$.

#### c) Poisson Distribution

For $X \sim \text{Poisson}(\lambda)$ where $P(X=0) = e^{-\lambda}$, find the MLE of $P(X=0)$ if $X_1, ..., X_n$ is a random sample from a Poisson($\lambda$) population.

#### MLE of μ

For a random sample $X_1, ..., X_n$ from a $N(\mu, 1)$ population, find the MLE of $\mu$.

#### MLE of σ

For a random sample $X_1, ..., X_n$ from a $N(0, \sigma^2)$ population, find the MLE of $\sigma$.

#### MLEs of μ and σ

For a random sample $X_1, ..., X_n$ from a $N(\mu, \sigma^2)$ population, find the MLEs of $\mu$ and $\sigma$. (Hint: Compute partial derivatives with respect to $\mu$ and $\sigma$ and set them equal to 0 to find the values $\hat{\mu}$ and $\hat{\sigma}$ that maximize the likelihood function.)

#### Likelihood Function
The likelihood function, denoted as $L(\theta | x)$, is a fundamental concept in statistics which measures how likely a set of observed data is under a particular statistical model parameterized by $\theta$.

#### Probability Plots
Probability plots are useful tools for determining if a random sample of data could have come from a specified population distribution. By analyzing the sample data and comparing it to theoretical distributions, probability plots help in identifying the most appropriate distribution that fits the data.

#### Introduction to Data Assignment

Given a sample of size $n=5$, we need to assign increasing, evenly spaced values between 0 and 1 to $X_i$. One simple method is to use $(i-0.5)/n$ for assignment.

#### Data Assignment Table

| $i$ | $X_i$ | $(i-0.5)/5$ |
| --- | ----- | ----------- |
| 1   | 3.01  | 0.1         |
| 2   | 3.35  | 0.33        |
| 3   | 4.79  | 0.54        |
| 4   | 5.96  | 0.75        |
| 5   | 7.89  | 0.9         |

#### Explanation of Assignment

The value $(i-0.5)/n$ represents the position of $X_i$ in the ordered sample. It accounts for the number of values less than $X_i$ and those less than or equal to $X_i$.

#### Normal Distribution Analysis

To determine if the sample could be from a normal population, we consider a normal distribution with mean and standard deviation equal to the sample mean and standard deviation. In this case, the sample mean is $X=5.00$ and the sample standard deviation is $s=2.00$.


#### Concepts:

1. **Normal Distribution (N(5,22))**:
   $N(5,22)$ distribution represents a normal distribution with a mean of 5 and a variance of 22. The cumulative distribution function (cdf) $F(x)$ of this distribution gives the probability of $X$ being less than or equal to $x$.

2. **Percentiles**:
   Percentiles represent the values below which a given percentage of observations in a distribution fall. In this context, $Q1$ (also known as the 10th percentile) denotes the value at which 10% of the values in the $N(5,22)$ distribution are less than or equal to $Q1$.

3. **Sample Analysis**:
   When analyzing samples from a distribution, if the sample points $(X1, ..., X5)$ closely align with the cdf of the $N(5,22)$ distribution, it suggests that the sample may have originated from this distribution. Horizontal lines drawn through sample points indicate the points where the sample intersects the cdf.

#### Relationship between Data Points and Percentiles
In a sample, the lowest value $X_1$ is expected to be close to the 10th percentile $Q_1$ of the population. This is because the lowest value in a sample is likely to come from the lowest portion of the population.

#### Expectation for Other Data Points
Similarly, each data point $X_i$ in the sample is expected to be close to its corresponding percentile $Q_i$ if the sample is representative of the population.

#### Normal Probability Plot
The points ($X_i, Q_i$) form a probability plot. If the points align closely with a diagonal line, it indicates that the distribution of $Q_i$ is normal, leading to a normal probability plot.

#### Alignment with Population Distribution
If $X_1, ..., X_n$ indeed represent the distribution that generated $Q_i$, the points on the probability plot will exhibit a pattern aligned with the normal distribution assumption.

